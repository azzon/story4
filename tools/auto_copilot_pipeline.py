#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
400ä¸‡å­—ç½‘æ–‡AIåˆ›ä½œç³»ç»Ÿ Â· è‡ªåŠ¨åŒ– Copilot Pipeline (Optimized V2)
===============================================================

æœ¬è„šæœ¬ç”¨äºè‡ªåŠ¨åŒ–æ‰§è¡Œ Stage TODO æ¸…å•ï¼Œé€šè¿‡ GitHub Issues + Copilot å®Œæˆæ•´ä¸ªåˆ›ä½œæµç¨‹ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ‰«æ todo/Stage-*.todos.md æ–‡ä»¶ï¼Œæå–æœªå‹¾é€‰çš„ TODO
2. ä¸ºæ¯ä¸ª TODO æˆ– TODO æ‰¹æ¬¡åˆ›å»º GitHub Issueï¼ˆåŒ…å«å®Œæ•´çš„æ‰§è¡ŒæŒ‡ä»¤ï¼‰
3. é€šè¿‡ Issue Assignment è§¦å‘ GitHub Copilot è‡ªåŠ¨æ‰§è¡Œ
4. ç›‘æ§ Copilot åˆ›å»ºçš„ PR çŠ¶æ€ï¼Œç­‰å¾… copilot_work_finished ä¿¡å·
5. è‡ªåŠ¨æ‰¹å‡†å¹¶åˆå¹¶ PRï¼Œå…³é—­ Issue
6. å¾ªç¯æ‰§è¡Œç›´åˆ°æ‰€æœ‰ TODO å®Œæˆ

å…³é”®ä¿®å¤ï¼š
- ä½¿ç”¨æ­£ç¡®çš„ Copilot è§¦å‘æœºåˆ¶ï¼šIssue Assignmentï¼ˆè€Œéè¯„è®ºï¼‰
- å°†æ‰€æœ‰ä»»åŠ¡è¯¦æƒ…å’Œæ‰§è¡ŒæŒ‡ä»¤æ”¾å…¥ Issue Bodyï¼ˆCopilot åªè¯»å–åˆå§‹æè¿°ï¼‰
- ä½¿ç”¨æ­£ç¡®çš„ bot åç§° "Copilot"ï¼ˆgh CLI å¯è¯†åˆ«ï¼‰
- ç§»é™¤æ— æ•ˆçš„è¯„è®ºè§¦å‘é€»è¾‘

"""

from __future__ import annotations

import argparse
import json
import logging
import re
import shutil
import signal
import subprocess
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Optional
from urllib.parse import urlparse

# ==================== é¡¹ç›®é…ç½® ====================

ROOT = Path(__file__).resolve().parents[1]
TODO_ROOT = ROOT / "todo"


def _extract_owner_repo(path: str) -> Optional[tuple[str, str]]:
    parts = [segment for segment in path.strip("/").split("/") if segment]
    if len(parts) >= 2:
        owner, repo = parts[-2], parts[-1]
        return owner, repo
    return None


def detect_repo_from_git() -> Optional[tuple[str, str]]:
    """å°è¯•ä» git remote ä¸­è§£æ owner/repoï¼Œå…¼å®¹å¤šç§ URL æ ¼å¼"""
    try:
        result = subprocess.run(
            ["git", "config", "--get", "remote.origin.url"],
            capture_output=True,
            text=True,
            check=True,
            encoding="utf-8"
        )
    except (subprocess.CalledProcessError, FileNotFoundError):
        return None

    url = result.stdout.strip()
    if not url:
        return None

    url = url.rstrip("/")
    if url.endswith(".git"):
        url = url[:-4]

    if url.startswith("git@"):
        path = url.split(":", 1)[1]
        return _extract_owner_repo(path)

    parsed = urlparse(url)
    if parsed.scheme in {"https", "http", "ssh", "git"}:
        return _extract_owner_repo(parsed.path)

    # å…¼å®¹ç±»ä¼¼ github.com/owner/repo æˆ– file åè®®çš„è·¯å¾„
    if ":" not in url and "/" in url:
        return _extract_owner_repo(url)

    return None


def resolve_repo() -> tuple[str, str]:
    detected = detect_repo_from_git()
    if not detected:
        raise RuntimeError(
            "æ— æ³•é€šè¿‡ git remote.origin.url è‡ªåŠ¨æ£€æµ‹ä»“åº“ä¿¡æ¯ï¼Œè¯·åœ¨ä»“åº“ä¸­é…ç½® remote.origin åå†è¿è¡Œã€‚"
        )
    return detected

# GitHub Copilot bot çš„åç§°é…ç½®
# ä¸åŒç³»ç»Ÿå’Œ gh CLI ç‰ˆæœ¬å¯èƒ½ä½¿ç”¨ä¸åŒçš„åç§°æ ¼å¼
COPILOT_ASSIGNEES = ["@copilot", "copilot", "Copilot", "github-copilot", "github-copilot[bot]"]  # æŒ‰ä¼˜å…ˆçº§å°è¯•
COPILOT_USERNAME = "copilot"

# è½®è¯¢é…ç½®
DEFAULT_POLL_INTERVAL = 60  # ç§’
DEFAULT_MAX_WAIT = 36000  # å•ä¸ª Issue æœ€å¤§ç­‰å¾…æ—¶é—´ï¼š10å°æ—¶
DEFAULT_BATCH_SIZE = 1  # æ¯ä¸ª Issue åŒ…å«çš„ TODO æ•°é‡ï¼ˆåŸå­æ‰§è¡Œï¼‰
DEFAULT_TASK_MAX_RETRIES = 3  # ä»»åŠ¡å¤±è´¥é‡è¯•æ¬¡æ•°
DEFAULT_TASK_RETRY_WAIT = 300  # ä»»åŠ¡é‡è¯•ç­‰å¾…æ—¶é—´ï¼š5åˆ†é’Ÿ
DEFAULT_MAX_PR_RESETS = 3  # å•ä¸ª Issue å†…æœ€å¤§ PR é‡ç½®æ¬¡æ•°
PR_TIMEOUT = 10800  # PR å¤„ç†è¶…æ—¶ï¼š3å°æ—¶
PR_WAIT_TIMEOUT = 1800  # ç­‰å¾… PR åˆ›å»ºè¶…æ—¶ï¼š30åˆ†é’Ÿ
RESET_WAIT_TIME = 30  # é‡ç½®åçš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
HEARTBEAT_INTERVAL = 300  # é•¿æ—¶é—´ç­‰å¾…æ—¶çš„å¿ƒè·³æ—¥å¿—é—´éš”ï¼ˆç§’ï¼‰
GH_TIMEOUT = 180  # GitHub CLI å‘½ä»¤è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œä» 120 å¢åŠ åˆ° 180
RETRY_SLEEP_SHORT = 5  # çŸ­æš‚é‡è¯•ç­‰å¾…ï¼ˆç§’ï¼‰
NETWORK_ERROR_BASE_WAIT = 10  # ç½‘ç»œé”™è¯¯åŸºç¡€ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
NETWORK_ERROR_MAX_WAIT = 120  # ç½‘ç»œé”™è¯¯æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
PR_READY_WAIT = 2  # PR æ ‡è®° Ready åç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
MAIN_ERROR_WAIT = 30  # ä¸»å¾ªç¯é”™è¯¯é‡è¯•ç­‰å¾…ï¼ˆç§’ï¼‰
TIMELINE_ACCEPT_HEADER = "Accept: application/vnd.github.mockingbird-preview+json"

CORE_DOCUMENTS = {
    ROOT / "Project-Bible.md": "# Project Bible\n\n> æœ¬æ–‡ä»¶ç”± auto_copilot_pipeline.py è‡ªåŠ¨åˆ›å»ºï¼Œç”¨äºç»´æŠ¤ä¸–ç•Œè§‚ã€è§’è‰²ä¸ä¼ç¬”æ€»è´¦ã€‚\n\n",
    ROOT / "Risk-Ledger.md": "# Risk Ledger\n\n> æœ¬æ–‡ä»¶ç”± auto_copilot_pipeline.py è‡ªåŠ¨åˆ›å»ºï¼Œç”¨äºè®°å½•é£é™©ã€å†³ç­–ä¸åç»­åŠ¨ä½œã€‚\n\n"
}

# ==================== æ­£åˆ™è¡¨è¾¾å¼ ====================

STAGE_FILE_PATTERN = re.compile(r"^Stage-(\d+)_(.+)\.todos\.md$")
# ä¿®æ­£ï¼šTODO è¡Œå®é™…æ˜¯ ### å¼€å¤´ï¼ˆä¸‰çº§æ ‡é¢˜ + åˆ—è¡¨é¡¹ï¼‰
# ä½¿ç”¨éè´ªå©ªåŒ¹é…é¿å… title ä¸­åŒ…å« ] å¯¼è‡´è§£æå¤±è´¥
TODO_LINE_PATTERN = re.compile(
    r"^###\s+-\s*\[(?P<status>[ xX])\]\s+\[(?P<todo_id>[^\]]+?)\]\s+(?P<title>.+)$"
)

# ==================== æ—¥å¿—é…ç½® ====================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("copilot-pipeline")


def ensure_core_documents() -> None:
    created: List[str] = []
    for path, placeholder in CORE_DOCUMENTS.items():
        if not path.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
            path.write_text(placeholder, encoding="utf-8")
            created.append(path.name)
    if created:
        logger.info("è‡ªåŠ¨åˆ›å»ºå…³é”®æ–‡æ¡£: %s", ", ".join(created))

# ==================== æ•°æ®æ¨¡å‹ ====================

@dataclass
class TodoItem:
    id_full: str
    stage_number: int
    title: str
    meta_lines: List[str]
    file_path: Path

    @property
    def stage_code(self) -> str:
        return f"{self.stage_number:02d}"

@dataclass
class WorkItem:
    id_full: str
    stage_number: int
    title: str
    file_path: Path
    todos: List[TodoItem]
    batch_index: Optional[int] = None
    batch_total: Optional[int] = None

    @property
    def stage_code(self) -> str:
        return f"{self.stage_number:02d}"

    @property
    def is_batch(self) -> bool:
        return len(self.todos) > 1

# ==================== Issue æ¨¡æ¿ ====================

ISSUE_BODY_TEMPLATE = """## ğŸ“‹ ä»»åŠ¡æ¦‚è§ˆ

{task_overview}

---

## ğŸ¯ æ‰§è¡Œè¦æ±‚

è¯·ä¸¥æ ¼æŒ‰ç…§ `.github/copilot-instructions.md` ä¸­çš„**ç»Ÿä¸€è‡ªåŠ¨åŒ–æµæ°´çº¿ (The Unified Loop)** æ‰§è¡Œï¼š

### 1. å¯»æ ‡ (Scan)
- ğŸ“ è¯»å– `{stage_file}`
- ğŸ” å®šä½åˆ°æŒ‡å®šçš„ TODO é¡¹{plural}
- ğŸ“– è¯»å–ä¸‹æ–¹çš„ä»»åŠ¡è¯¦æƒ…å’Œå…ƒä¿¡æ¯

### 2. ä¸“å®¶è®®ä¼š (Council & Think)
- ğŸ‘¥ ç»„å»º 3 äººä¸“å®¶å°ç»„ï¼ˆä¸»ç†äºº + å•†ä¸šé¡¾é—® + é£æ§ï¼‰
- ğŸ” æ£€ç´¢ç›¸å…³æ–‡ä»¶ï¼ˆProject-Bible.md ç­‰ï¼‰
- âš ï¸ å†²çªæ£€æŸ¥ï¼šæ–°æ„æ€æ˜¯å¦è¿èƒŒæ—§è®¾å®šï¼Ÿ
- ğŸ’° ä»·å€¼è¯„ä¼°ï¼šç¬¦åˆåŒ—ææ˜ŸæŒ‡æ ‡å—ï¼Ÿ
- ğŸ”¬ æ·±åº¦æŒ–æ˜ï¼šæŒ–æ˜æ‰€æœ‰å¯èƒ½çš„åˆ†æ”¯å’Œç»†èŠ‚

### 3. è§„åˆ’ (Plan)
- ğŸ“‚ ç¡®å®šè¾“å‡ºè·¯å¾„ï¼š`archives/Stage-{stage_code}_*/{{Filename}}.md`
- ğŸ“‹ ç¡®å®šä¾èµ–æ–‡ä»¶
- ğŸ”„ ç¡®å®šæ˜¯å¦éœ€è¦æ›´æ–° Project-Bible.md

### 4. ç”Ÿäº§ (Draft)
- âœï¸ è¾“å‡º**è¯¦å°½å®Œæ•´**çš„å†…å®¹ï¼Œä¸¥ç¦çœç•¥
- ğŸš« æ‰§è¡Œå» AI å‘³åè®®ï¼ˆç¦ç”¨ï¼šç„¶è€Œã€æ˜¾ç„¶ã€å°±åœ¨è¿™æ—¶ç­‰ï¼‰
- âœ… ä½¿ç”¨ Show-Don't-Tellï¼Œå¼ºåˆ¶çŸ­å¥
- ğŸ£ æ¯ 2000 å­—æ£€æŸ¥å•†ä¸šé’©å­

### 5. è´¨æ£€ (Verify)
- ğŸ” è‡ªæˆ‘å®¡è§†ï¼šæ»¡è¶³éªŒæ”¶æ ‡å‡†å—ï¼Ÿ
- ğŸ¤” åƒäººç±»å¤§ç¥å†™çš„å—ï¼Ÿ
- ğŸ” ä¸æ»¡æ„ç«‹å³ #redoï¼Œä¸è¦é—®ç”¨æˆ·

### 6. å½’æ¡£ (Commit)
- ğŸ’¾ ä¿å­˜æ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„
- ğŸ“ æ›´æ–° `Project-Bible.md`ï¼ˆå¦‚æœ‰æ–°è®¾å®š/ä¼ç¬”/è§’è‰²ï¼‰
- âš ï¸ æ›´æ–° `Risk-Ledger.md`ï¼ˆå¦‚æœ‰æœªå†³é—®é¢˜ï¼‰
- âœ… å‹¾é€‰ TODOï¼šå°† `- [ ]` æ”¹ä¸º `- [x]`

---

## âš ï¸ ç»å¯¹ç¡¬çº¦æŸ

1. **åŸå­åŒ–æ‰§è¡Œ**ï¼šæ¯ä¸ª TODO ç‹¬ç«‹å®Œæˆï¼Œä¸¥ç¦æ‰¹é‡å‹¾é€‰
2. **é—­ç¯äº¤ä»˜**ï¼šå¿…é¡»æœ‰å®è´¨æ€§äº§å‡ºï¼Œç¦æ­¢"ç•¥"ã€"å¾…è¡¥å……"
3. **æ·±åº¦æ€è€ƒ**ï¼šæ‹’ç»æ•·è¡ï¼Œæœ€å¤§åŒ– AI ç®—åŠ›
4. **å¼ºåˆ¶ä¸­æ–‡**ï¼šæ‰€æœ‰è¾“å‡ºä½¿ç”¨ç®€ä½“ä¸­æ–‡

---

## ğŸ“¦ äº¤ä»˜æ ‡å‡†

- [ ] TODO å·²å‹¾é€‰ï¼ˆä¿®æ”¹ `{stage_file}`ï¼‰
- [ ] äº§å‡ºå·²å½’æ¡£ï¼ˆä¿å­˜åˆ° `archives/` ç›®å½•ï¼‰
- [ ] è®¾å®šå·²æ›´æ–°ï¼ˆåŒæ­¥åˆ° Project-Bible.mdï¼‰
- [ ] PR åŒ…å«è‡ªæ£€æ¸…å•
- [ ] PR æè¿°åŒ…å« `Fixes #{{issue_number}}`

---

## ğŸ“š å‚è€ƒæ–‡ä»¶

{reference_files}

---

## ğŸ’¡ æç¤º

- å¦‚é‡åˆ°éé˜»æ–­æ€§é—®é¢˜ï¼Œä¾æ®è®¾å®šåº“è‡ªè¡Œå†³ç­–å¹¶è®°å½•æ—¥å¿—
- æ¯ä¸ª TODO éƒ½æ˜¯ç‹¬ç«‹çš„åˆ›ä½œè¡Œä¸ºï¼Œéœ€è¦å®Œæ•´çš„å…­æ­¥å¾ªç¯
- ä¸è¦åå•¬ Tokenï¼Œåœ¨ #think å’Œ #draft é˜¶æ®µå°½å¯èƒ½è¯¦å°½
"""

# ==================== è¾…åŠ©å‡½æ•° ====================

def extract_stage_number_from_filename(path: Path) -> int:
    match = STAGE_FILE_PATTERN.match(path.name)
    if not match:
        raise ValueError(f"æ— æ³•è§£æ Stage ç¼–å·: {path.name}")
    return int(match.group(1))

def stage_file_sort_key(path: Path) -> tuple[int, str]:
    return extract_stage_number_from_filename(path), path.name

# ==================== GitHub å®¢æˆ·ç«¯ ====================

class GitHubClient:
    def __init__(self, owner: str, repo: str) -> None:
        self.owner = owner
        self.repo = repo
        self.repo_ref = f"{owner}/{repo}"
        if not shutil.which("gh"):
            raise RuntimeError("æœªæ‰¾åˆ° gh CLI")

    def _run_gh(self, args: List[str], retries: int = 3) -> str:
        cmd = ["gh"] + args
        for attempt in range(1, retries + 1):
            try:
                result = subprocess.run(
                    cmd, capture_output=True, text=True, check=True,
                    encoding="utf-8", timeout=GH_TIMEOUT
                )
                return result.stdout.strip()
            except subprocess.TimeoutExpired:
                if attempt == retries:
                    logger.error(f"gh å‘½ä»¤æœ€ç»ˆè¶…æ—¶å¤±è´¥ï¼ˆ{retries} æ¬¡å°è¯•ï¼‰: {' '.join(args[:3])}...")
                    raise RuntimeError(f"gh å‘½ä»¤è¶…æ—¶: {' '.join(args)}")
                wait_time = min(2 ** attempt * NETWORK_ERROR_BASE_WAIT, NETWORK_ERROR_MAX_WAIT)
                logger.warning(f"gh å‘½ä»¤è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯• ({attempt}/{retries}): {' '.join(args[:3])}...")
                time.sleep(wait_time)
            except subprocess.CalledProcessError as exc:
                stderr = exc.stderr.strip() if exc.stderr else "æ— é”™è¯¯ä¿¡æ¯"

                # æ£€æµ‹ç½‘ç»œç›¸å…³é”™è¯¯
                NETWORK_ERRORS = {"tls handshake", "bad gateway", "connection reset",
                                 "connection refused", "network", "timeout", "eof",
                                 "502", "503", "504"}
                is_network_error = any(keyword in stderr.lower() for keyword in NETWORK_ERRORS)

                # æœ€åä¸€æ¬¡å°è¯•ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                if attempt == retries:
                    error_type = "ç½‘ç»œé”™è¯¯" if is_network_error else "å‘½ä»¤é”™è¯¯"
                    logger.error(f"gh {error_type}æœ€ç»ˆå¤±è´¥ï¼ˆ{retries} æ¬¡å°è¯•ï¼‰: {stderr[:200]}")
                    raise RuntimeError(f"gh å‘½ä»¤å¤±è´¥: {stderr}") from exc

                # æŸäº›é”™è¯¯ä¸å€¼å¾—é‡è¯•ï¼ˆç«‹å³å¤±è´¥ï¼‰
                if "not found" in stderr.lower() or "unknown" in stderr.lower():
                    logger.error(f"gh å‘½ä»¤è‡´å‘½é”™è¯¯ï¼ˆä¸å¯é‡è¯•ï¼‰: {stderr}")
                    raise RuntimeError(f"gh å‘½ä»¤é”™è¯¯: {stderr}") from exc

                # é’ˆå¯¹ Rate Limit çš„ç‰¹æ®Šå¤„ç†
                if "rate limit" in stderr.lower() or "abuse" in stderr.lower():
                    wait_time = min(300 * attempt, 1800)  # æœ€å¤šç­‰å¾… 30 åˆ†é’Ÿ
                    logger.warning(f"GitHub API é™æµè­¦å‘Šï¼Œæš‚åœ {wait_time}ç§’ ({wait_time/60:.1f}min) åé‡è¯• ({attempt}/{retries})")
                    time.sleep(wait_time)
                # ç½‘ç»œé”™è¯¯ä½¿ç”¨æŒ‡æ•°é€€é¿
                elif is_network_error:
                    wait_time = min(2 ** attempt * NETWORK_ERROR_BASE_WAIT, NETWORK_ERROR_MAX_WAIT)
                    logger.warning(f"ç½‘ç»œé”™è¯¯ï¼Œ{wait_time}ç§’åé‡è¯• ({attempt}/{retries}): {stderr[:100]}")
                    time.sleep(wait_time)
                else:
                    wait_time = min(2 ** attempt, 30)
                    logger.warning(f"gh å‘½ä»¤å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯• ({attempt}/{retries}): {stderr[:100]}")
                    time.sleep(wait_time)

        raise RuntimeError(f"gh å‘½ä»¤å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯ (é‡è¯• {retries} æ¬¡åä»å¤±è´¥)")

    def api_request(self, method: str, path: str, headers: Optional[List[str]] = None,
                   silent_fail: bool = False) -> Any:
        """å‘èµ· GitHub API è¯·æ±‚

        Args:
            method: HTTP æ–¹æ³•
            path: API è·¯å¾„
            headers: å¯é€‰çš„ HTTP å¤´
            silent_fail: å¦‚æœä¸º Trueï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸ï¼›å¦åˆ™æŠ›å‡ºå¼‚å¸¸
        """
        if path.startswith("https://api.github.com"):
            path = path.replace("https://api.github.com", "")

        args = ["api", path, "--method", method]
        if headers:
            for header in headers:
                args.extend(["-H", header])

        try:
            output = self._run_gh(args)
            return json.loads(output) if output else {}
        except Exception as exc:
            if silent_fail:
                logger.debug(f"GitHub API è¯·æ±‚å¤±è´¥ ({method} {path}): {exc}")
                return {}
            else:
                logger.warning(f"GitHub API è¯·æ±‚å¤±è´¥ ({method} {path}): {exc}")
                raise

    def create_issue(self, title: str, body: str) -> int:
        args = ["issue", "create", "--repo", self.repo_ref, "--title", title, "--body", body]
        output = self._run_gh(args)
        try:
            # gh CLI è¿”å› Issue URLï¼Œæå–æœ€åçš„æ•°å­—
            issue_num = int(output.strip().split('/')[-1])
            if issue_num <= 0:
                raise ValueError(f"æ— æ•ˆçš„ Issue ç¼–å·: {issue_num}")
            return issue_num
        except (ValueError, IndexError) as e:
            raise RuntimeError(f"è§£æ Issue ç¼–å·å¤±è´¥ï¼Œè¾“å‡º: {output}") from e

    def add_assignees(self, issue_number: int, assignees: List[str]) -> None:
        """åˆ†é… Issue ç»™æŒ‡å®šç”¨æˆ·/Botï¼Œå°è¯•å¤šä¸ªåç§°æ ¼å¼"""
        last_error = None
        for assignee in assignees:
            try:
                self._run_gh([
                    "issue", "edit", str(issue_number),
                    "--repo", self.repo_ref, "--add-assignee", assignee
                ])
                logger.debug(f"æˆåŠŸåˆ†é… Issue #{issue_number} ç»™ {assignee}")
                return  # æˆåŠŸåˆ™ç«‹å³è¿”å›
            except RuntimeError as e:
                last_error = e
                logger.debug(f"å°è¯•åˆ†é…ç»™ {assignee} å¤±è´¥: {e}")
                continue
            except Exception as e:
                last_error = e
                logger.debug(f"å°è¯•åˆ†é…ç»™ {assignee} æ—¶å¼‚å¸¸: {e}")
                continue

        # æ‰€æœ‰åç§°éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
        if last_error:
            raise RuntimeError(f"æ— æ³•åˆ†é… Issue #{issue_number} ç»™ Copilotï¼ˆå·²å°è¯• {len(assignees)} ä¸ªåç§°ï¼‰: {last_error}") from last_error

    def remove_assignees(self, issue_number: int, assignees: List[str]) -> None:
        """å–æ¶ˆåˆ†é… Issue çš„æŒ‡å®šç”¨æˆ·/Bot"""
        for assignee in assignees:
            try:
                self._run_gh([
                    "issue", "edit", str(issue_number),
                    "--repo", self.repo_ref, "--remove-assignee", assignee
                ])
                logger.debug(f"æˆåŠŸå–æ¶ˆåˆ†é… Issue #{issue_number} çš„ {assignee}")
            except Exception as e:
                # å–æ¶ˆåˆ†é…å¤±è´¥ä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œå¯èƒ½è¯¥ç”¨æˆ·æœ¬æ¥å°±æ²¡åˆ†é…
                logger.debug(f"å–æ¶ˆåˆ†é… {assignee} å¤±è´¥ï¼ˆå¯èƒ½æœªåˆ†é…ï¼‰: {e}")

    def comment_issue(self, issue_number: int, body: str) -> None:
        self._run_gh(["issue", "comment", str(issue_number), "--repo", self.repo_ref, "--body", body])

    def get_issue(self, issue_number: int) -> dict:
        """è·å– Issue ä¿¡æ¯ï¼ŒåŒ…å«çŠ¶æ€å’Œåˆ†é…è€…"""
        output = self._run_gh([
            "issue", "view", str(issue_number), "--repo", self.repo_ref,
            "--json", "number,state,assignees"
        ])
        try:
            return json.loads(output) if output else {}
        except json.JSONDecodeError as e:
            raise RuntimeError(f"è§£æ Issue æ•°æ®å¤±è´¥: {e}") from e

    def get_pull(self, pr_number: int) -> dict:
        output = self._run_gh([
            "pr", "view", str(pr_number), "--repo", self.repo_ref,
            "--json", "number,state,mergedAt,isDraft,updatedAt,files"
        ])
        try:
            data = json.loads(output) if output else {}
        except json.JSONDecodeError as e:
            raise RuntimeError(f"è§£æ PR æ•°æ®å¤±è´¥: {e}") from e

        # ç»Ÿä¸€å­—æ®µåï¼ˆPython é£æ ¼ï¼‰
        if "mergedAt" in data:
            data["merged_at"] = data.pop("mergedAt")
        if "isDraft" in data:
            data["draft"] = data.pop("isDraft")
        return data

    def merge_pull(self, pr_number: int) -> dict:
        self._run_gh(["pr", "merge", str(pr_number), "--repo", self.repo_ref, "--squash", "--delete-branch"])
        return {"merged": True}

    def close_pr(self, pr_number: int, delete_branch: bool = True) -> None:
        """å…³é—­ PR å¹¶å¯é€‰åˆ é™¤åˆ†æ”¯

        æ³¨æ„ï¼šå…³é—­ PRï¼ˆæœªåˆå¹¶ï¼‰ä¸ä¼šè‡ªåŠ¨å…³é—­å…³è”çš„ Issueï¼Œ
        å³ä½¿ PR æè¿°ä¸­åŒ…å« 'Fixes #123'ã€‚
        åªæœ‰åˆå¹¶ PR æ‰ä¼šè§¦å‘ Issue çš„è‡ªåŠ¨å…³é—­ã€‚
        """
        args = ["pr", "close", str(pr_number), "--repo", self.repo_ref]
        if delete_branch:
            args.append("--delete-branch")
        self._run_gh(args)

    def list_closed_issues(self, limit: int = 1000) -> List[dict]:
        """æŸ¥è¯¢å·²å…³é—­çš„ Issues"""
        output = self._run_gh([
            "issue", "list", "--repo", self.repo_ref,
            "--state", "closed",
            "--limit", str(limit), "--json", "title"
        ])
        try:
            issues = json.loads(output) if output else []
            return issues if isinstance(issues, list) else []
        except json.JSONDecodeError:
            return []

    def find_issue_by_todo(self, todo_id: str) -> Optional[int]:
        """å°è¯•æ ¹æ®æ ‡é¢˜ä¸­çš„ TODO ID æŸ¥æ‰¾å·²æœ‰çš„å¼€æ”¾ Issue"""
        if not todo_id or not todo_id.strip():
            return None

        try:
            output = self._run_gh([
                "issue", "list", "--repo", self.repo_ref,
                "--state", "open",
                "--search", f"[{todo_id}]",
                "--json", "number,title"
            ], retries=2)
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢ç°æœ‰ Issue å¤±è´¥: {e}")
            return None

        try:
            issues = json.loads(output) if output else []
            if not isinstance(issues, list):
                logger.warning(f"Issue æŸ¥è¯¢è¿”å›äº†éåˆ—è¡¨æ•°æ®: {type(issues)}")
                return None
        except json.JSONDecodeError as e:
            logger.warning(f"è§£æ Issue åˆ—è¡¨å¤±è´¥: {e}")
            return None

        for issue in issues:
            if not isinstance(issue, dict):
                continue
            title = issue.get("title", "")
            issue_num = issue.get("number")
            if f"[{todo_id}]" in title and isinstance(issue_num, int):
                return issue_num
        return None

    def latest_pr_from_timeline(self, issue_number: int) -> Optional[int]:
        events = self.api_request(
            "GET",
            f"/repos/{self.repo_ref}/issues/{issue_number}/timeline",
            headers=[TIMELINE_ACCEPT_HEADER],
            silent_fail=True  # timeline æŸ¥è¯¢å¤±è´¥ä¸åº”ç»ˆæ­¢æµç¨‹
        )
        if not isinstance(events, list): return None
        for event in reversed(events):
            if event.get("event") == "cross-referenced":
                source = event.get("source", {}).get("issue", {})
                if "pull_request" in source:
                    return source["number"]
        return None

    def mark_pr_ready(self, pr_number: int) -> None:
        """å°† PR æ ‡è®°ä¸º Ready çŠ¶æ€

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¼šå°† Draft PR æˆ– "Ready for Review" çŠ¶æ€çš„ PR è½¬ä¸º Readyã€‚
        å¦‚æœ PR å·²ç»æ˜¯ Ready çŠ¶æ€ï¼Œå‘½ä»¤å¯èƒ½ä¼šå¤±è´¥ï¼Œè¿™æ˜¯æ­£å¸¸è¡Œä¸ºã€‚
        """
        try:
            self._run_gh(["pr", "ready", str(pr_number), "--repo", self.repo_ref])
            logger.debug(f"æˆåŠŸè°ƒç”¨ gh pr ready #{pr_number}")
        except Exception as e:
            error_msg = str(e).lower()
            # å¦‚æœé”™è¯¯ä¿¡æ¯è¡¨æ˜ PR å·²ç» readyï¼Œè¿™ä¸æ˜¯çœŸæ­£çš„é”™è¯¯
            if "already" in error_msg or "not a draft" in error_msg:
                logger.debug(f"PR #{pr_number} å·²å¤„äº Ready çŠ¶æ€")
            else:
                logger.warning(f"æ ‡è®° PR #{pr_number} ä¸º Ready å¤±è´¥: {e}")

# ==================== PR ç›‘æ§ ====================

def check_copilot_signal(github: GitHubClient, pr_number: int) -> bool:
    """æ£€æŸ¥ PR ä¸­æ˜¯å¦æœ‰ copilot_work_finished äº‹ä»¶"""
    events = github.api_request(
        "GET",
        f"/repos/{github.repo_ref}/issues/{pr_number}/timeline",
        headers=[TIMELINE_ACCEPT_HEADER],
        silent_fail=True  # ä¿¡å·æ£€æµ‹å¤±è´¥æ—¶è¿”å› False è€Œä¸æ˜¯æŠ›å¼‚å¸¸
    )

    if isinstance(events, list):
        for event in reversed(events):
            if event.get("event") == "copilot_work_finished":
                return True
    return False

# ==================== è§£æå™¨ ====================

def parse_stage_structure(path: Path) -> tuple[int, List[TodoItem]]:
    stage_num = extract_stage_number_from_filename(path)
    logger.info(f"è§£æ {path.name}")

    if not path.exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return stage_num, []

    if not path.is_file():
        logger.error(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {path}")
        return stage_num, []

    try:
        lines = path.read_text(encoding="utf-8").splitlines()
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {path}: {e}")
        return stage_num, []

    todos = []
    idx = 0
    total_lines = len(lines)

    while idx < total_lines:
        line = lines[idx]
        match = TODO_LINE_PATTERN.match(line)
        if not match:
            idx += 1
            continue

        # è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
        if match.group("status").lower() == "x":
            logger.debug(f"è·³è¿‡å·²å®Œæˆä»»åŠ¡: {match.group('todo_id')}")
            # è·³è¿‡è¯¥ä»»åŠ¡çš„å…ƒä¿¡æ¯å—ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ª TODO æˆ– Group Header
            idx += 1
            while idx < total_lines:
                next_line = lines[idx]
                if TODO_LINE_PATTERN.match(next_line) or (next_line.strip().startswith('##') and 'Group' in next_line):
                    break
                idx += 1
            # idx ç°åœ¨æŒ‡å‘ä¸‹ä¸€ä¸ª TODO æˆ– Groupï¼Œç»§ç»­å¤–å±‚å¾ªç¯
            continue

        todo_id = match.group("todo_id").strip()
        title = match.group("title").strip()

        # æå–å…ƒä¿¡æ¯ï¼šè¯»å–ç›´åˆ°ä¸‹ä¸€ä¸ª TODO æˆ– Group Header
        meta = []
        j = idx + 1
        while j < total_lines:
            meta_line = lines[j]
            # æ£€æµ‹ä¸‹ä¸€ä¸ª TODO æˆ–åˆ†ç»„æ ‡é¢˜
            if TODO_LINE_PATTERN.match(meta_line) or (meta_line.strip().startswith('##') and 'Group' in meta_line):
                break
            meta.append(meta_line)
            j += 1

        # æ¸…ç†å°¾éƒ¨ç©ºè¡Œå’Œåˆ†éš”ç¬¦
        while meta and (not meta[-1].strip() or meta[-1].strip() == '---'):
            meta.pop()

        # å¦‚æœ meta ä¸ºç©ºï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯ä½†ç»§ç»­
        if not meta:
            logger.debug(f"TODO {todo_id} æ²¡æœ‰å…ƒä¿¡æ¯")
        else:
            logger.debug(f"TODO {todo_id}: {len(meta)} è¡Œå…ƒä¿¡æ¯")

        todos.append(TodoItem(todo_id, stage_num, title, meta, path))
        idx = j

    logger.info(f"è§£æå®Œæˆ: {len(todos)} ä¸ªå¾…åŠä»»åŠ¡")
    return stage_num, todos

def iter_work_items(todo_root: Path, batch_size: int, completed_ids: set[str]) -> List[WorkItem]:
    """è·å–å½“å‰éœ€è¦å¤„ç†çš„å·¥ä½œé¡¹

    æ³¨æ„ï¼šé‡‡ç”¨ Stage é”å®šæœºåˆ¶ï¼Œæ¯æ¬¡åªè¿”å›ä¸€ä¸ª Stage çš„ä»»åŠ¡ã€‚
    è¿™ç¡®ä¿äº†ä»»åŠ¡æŒ‰ Stage é¡ºåºæ‰§è¡Œï¼Œå®Œæˆä¸€ä¸ª Stage åå†å¤„ç†ä¸‹ä¸€ä¸ªã€‚

    Args:
        todo_root: TODO æ–‡ä»¶æ‰€åœ¨ç›®å½•
        batch_size: æ¯ä¸ª Issue åŒ…å«çš„ TODO æ•°é‡
        completed_ids: å·²å®Œæˆçš„ TODO ID é›†åˆ

    Returns:
        å½“å‰éœ€è¦å¤„ç†çš„å·¥ä½œé¡¹åˆ—è¡¨ï¼ˆåªåŒ…å«ä¸€ä¸ª Stageï¼‰
    """
    if not todo_root.exists():
        logger.warning(f"TODO ç›®å½•ä¸å­˜åœ¨: {todo_root}")
        return []

    if not todo_root.is_dir():
        logger.error(f"TODO è·¯å¾„ä¸æ˜¯ç›®å½•: {todo_root}")
        return []

    files = sorted(todo_root.glob("Stage-*.todos.md"), key=stage_file_sort_key)

    # ä¿®å¤ï¼šç¡®ä¿ batch_size è‡³å°‘ä¸º 1ï¼ˆåŸå­æ‰§è¡ŒåŸåˆ™ï¼‰
    batch_size = max(1, batch_size)

    for path in files:
        stage_num, todos = parse_stage_structure(path)
        if not todos:
            continue

        batches = [todos[i:i + batch_size] for i in range(0, len(todos), batch_size)]

        stage_items: List[WorkItem] = []
        for i, batch in enumerate(batches, 1):
            if len(batch) == 1:
                stage_items.append(WorkItem(batch[0].id_full, stage_num, batch[0].title, path, batch))
            else:
                wid = f"S{stage_num:02d}-BATCH-{i:02d}"
                title = f"{path.stem} æ‰¹æ¬¡ {i}/{len(batches)}"
                stage_items.append(WorkItem(wid, stage_num, title, path, batch, i, len(batches)))

        filtered_items: List[WorkItem] = []
        for item in stage_items:
            if item.id_full in completed_ids:
                logger.info(f"â­ è·³è¿‡å·²å®Œæˆä»»åŠ¡/æ‰¹æ¬¡ (GitHub): {item.id_full}")
                continue

            if item.is_batch:
                all_sub_completed = all(todo.id_full in completed_ids for todo in item.todos)
                if all_sub_completed:
                    logger.info(f"â­ è·³è¿‡å·²å®Œæˆæ‰¹æ¬¡ (å­ä»»åŠ¡å…¨æ¸…): {item.id_full}")
                    continue

            filtered_items.append(item)

        if filtered_items:
            logger.info(f"é”å®š Stage {stage_num:02d}ï¼Œå¾…å¤„ç† {len(filtered_items)} ä¸ªä»»åŠ¡")
            return filtered_items

    return []

# ==================== Pipeline ====================

class Pipeline:
    def __init__(self, github: Optional[GitHubClient], args: argparse.Namespace) -> None:
        self.github = github
        self.args = args

    def _require_github(self) -> GitHubClient:
        if not self.github:
            raise RuntimeError("GitHub å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œåœ¨çº¿æ“ä½œã€‚è¯·ç§»é™¤ --dry-run æˆ–æ­£ç¡®é…ç½® gh CLIã€‚")
        return self.github

    def get_recent_completed_todos(self, limit: int = 1000) -> set[str]:
        """è·å–æœ€è¿‘å·²å®Œæˆçš„ TODO ID é›†åˆï¼Œç”¨äºè‡ªåŠ¨è·³è¿‡

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¾èµ– GitHub APIï¼Œå¦‚æœéœ€è¦æ›´å¿«çš„å¯åŠ¨é€Ÿåº¦ï¼Œ
        å¯ä»¥è€ƒè™‘å°†è¿›åº¦ç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆå¦‚ .pipeline_progress.jsonï¼‰
        """
        if not self.github:
            logger.debug("è·³è¿‡è¿œç¨‹è¿›åº¦æ‰«æï¼šå½“å‰ä¸º dry-run æˆ– GitHub æœªé…ç½®")
            return set()

        github = self._require_github()

        try:
            # æŸ¥è¯¢æ‰€æœ‰å·²å…³é—­çš„ Issues
            issues = github.list_closed_issues(limit)
            if not issues:
                logger.debug("æœªä» GitHub è·å–åˆ°å·²å…³é—­çš„ Issue")
                return set()

            completed = set()
            for issue in issues:
                if not isinstance(issue, dict):
                    continue
                title = issue.get("title", "")
                if not title:
                    continue
                match = re.search(r"\[([^\]]+?)\]", title)
                if match:
                    todo_id = match.group(1).strip()
                    if todo_id:
                        completed.add(todo_id)

            if completed:
                logger.info(f"ä» GitHub è·å–åˆ° {len(completed)} ä¸ªæœ€è¿‘å®Œæˆçš„ä»»åŠ¡è®°å½•")
            else:
                logger.debug("æœªä» GitHub è·å–åˆ°å·²å®Œæˆçš„ä»»åŠ¡è®°å½•")
            return completed
        except json.JSONDecodeError as e:
            logger.warning(f"è§£æ Issue åˆ—è¡¨å¤±è´¥: {e}")
            return set()
        except Exception as e:
            logger.warning(f"è·å–å·²å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
            return set()

    def run(self, items: Iterable[WorkItem]) -> None:
        items_list = items if isinstance(items, list) else list(items)
        total = len(items_list)
        logger.info(f"\n{'='*80}")
        logger.info(f"å·¥ä½œé˜Ÿåˆ—ç»Ÿè®¡")
        logger.info(f"{'='*80}")
        logger.info(f"å¾…å¤„ç†å·¥ä½œé¡¹æ€»æ•°: {total}")
        if total == 0:
            logger.info("âœ“ æ‰€æœ‰ TODO å·²å®Œæˆï¼Œæ— éœ€è¿›ä¸€æ­¥æ“ä½œã€‚")
            return

        # æŒ‰ Stage åˆ†ç»„ç»Ÿè®¡
        stage_counts = {}
        for item in items_list:
            stage_counts[item.stage_number] = stage_counts.get(item.stage_number, 0) + 1
        for stage, count in sorted(stage_counts.items()):
            logger.info(f"  Stage {stage:02d}: {count} ä¸ªä»»åŠ¡")
        logger.info(f"{'='*80}\n")

        max_task_retries = max(1, self.args.task_max_retries)
        base_retry_wait = max(1, self.args.task_retry_wait)

        failed_items = []  # è®°å½•å¤±è´¥çš„ä»»åŠ¡

        for i, item in enumerate(items_list, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ“‹ è¿›åº¦: {i}/{total} ({i*100//total}%)")
            logger.info(f"ğŸ”– å·¥ä½œé¡¹: {item.id_full}")
            logger.info(f"ğŸ“ æ ‡é¢˜: {item.title}")
            if item.is_batch:
                logger.info(f"ğŸ“¦ æ‰¹æ¬¡: {item.batch_index}/{item.batch_total} (åŒ…å« {len(item.todos)} ä¸ªå­ä»»åŠ¡)")
            logger.info(f"{'='*80}")

            # ä»»åŠ¡çº§é‡è¯•æœºåˆ¶
            for attempt in range(1, max_task_retries + 1):
                try:
                    issue_num = self._ensure_issue(item)
                    if not self.args.dry_run:
                        self._wait_and_merge(item, issue_num)
                    logger.info(f"\nâœ“ [{i}/{total}] {item.id_full} å®Œæˆ\n")
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    logger.error(f"\nâœ— [{i}/{total}] {item.id_full} å¤±è´¥ (å°è¯• {attempt}/{max_task_retries}): {e}")
                    if attempt < max_task_retries:
                        wait_time = base_retry_wait * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿: base, 2*base, 4*base, ...
                        logger.warning(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        # é‡è¯•è€—å°½ï¼Œè®°å½•å¤±è´¥ä½†ç»§ç»­å¤„ç†åç»­ä»»åŠ¡
                        logger.error(f"âœ—âœ—âœ— [{i}/{total}] {item.id_full} æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡å¹¶ç»§ç»­åç»­ä»»åŠ¡")
                        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š")
                        failed_items.append((item.id_full, str(e)))
                        break

        # æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œåï¼ŒæŠ¥å‘Šå¤±è´¥çš„ä»»åŠ¡
        if failed_items:
            logger.error(f"\n{'='*80}")
            logger.error(f"âš ï¸  æœ‰ {len(failed_items)} ä¸ªä»»åŠ¡æœ€ç»ˆå¤±è´¥ï¼š")
            for task_id, error in failed_items:
                logger.error(f"  - {task_id}: {error}")
            logger.error(f"{'='*80}\n")
            logger.error("è¯·æ‰‹åŠ¨å¤„ç†å¤±è´¥çš„ä»»åŠ¡ï¼Œç„¶åé‡æ–°è¿è¡Œè„šæœ¬")

    def _ensure_issue(self, item: WorkItem) -> int:
        if self.args.dry_run:
            logger.info(f"[DRY RUN] åˆ›å»º Issue: {item.title}")
            return 0

        github = self._require_github()
        existing = github.find_issue_by_todo(item.id_full)

        # å°è¯•å¤ç”¨å·²æœ‰çš„ open Issue
        if existing:
            logger.info(f"æ£€æµ‹åˆ°çº¿ä¸Šå·²æœ‰ Issue #{existing}")
            try:
                issue_data = github.get_issue(existing)
                state = issue_data.get('state')
                logger.debug(f"Issue #{existing} çŠ¶æ€: {state}")

                if state == "open":
                    # Issue æ˜¯ open çŠ¶æ€ï¼Œç¡®ä¿ Copilot å·²åˆ†é…
                    assignees = {
                        (assignee.get("login") or "").lower()
                        for assignee in issue_data.get("assignees", []) if assignee
                    }
                    if COPILOT_USERNAME not in assignees:
                        github.add_assignees(existing, COPILOT_ASSIGNEES)
                        logger.info(f"âœ“ å·²å°† Issue #{existing} é‡æ–°åˆ†é…ç»™ Copilot")
                    else:
                        logger.debug(f"Issue #{existing} å·²åˆ†é…ç»™ Copilotï¼Œç›´æ¥å¤ç”¨")
                    return existing
                else:
                    # Issue å·²å…³é—­æˆ–æœªçŸ¥çŠ¶æ€ï¼Œåˆ›å»ºæ–°çš„
                    logger.warning(f"Issue #{existing} çŠ¶æ€ä¸º {state}ï¼Œå°†åˆ›å»ºæ–° Issue")
            except Exception as e:
                logger.warning(f"è·å– Issue #{existing} è¯¦æƒ…å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–° Issue")

        # æ„å»º Issue Body
        body = self._build_body(item)
        # æ„å»ºå®Œæ•´çš„ Issue Bodyï¼ˆåŒ…å«æ‰€æœ‰ä»»åŠ¡è¯¦æƒ…å’Œæ‰§è¡ŒæŒ‡ä»¤ï¼‰
        full_body = self._build_full_issue_body(item, body)

        issue_num = github.create_issue(f"[{item.id_full}] {item.title}", full_body)
        logger.info(f"åˆ›å»º Issue #{issue_num}")

        # å…³é”®ï¼šé€šè¿‡ Assignment è§¦å‘ Copilotï¼ˆè€Œä¸æ˜¯è¯„è®ºï¼‰
        logger.info(f"åˆ†é… Issue #{issue_num} ç»™ Copilot ä»¥è§¦å‘è‡ªåŠ¨æ‰§è¡Œ...")
        try:
            github.add_assignees(issue_num, COPILOT_ASSIGNEES)
            logger.info(f"âœ“ æˆåŠŸåˆ†é…ç»™ Copilot")
        except Exception as e:
            # åˆ†é…å¤±è´¥æ˜¯ä¸¥é‡é”™è¯¯ï¼Œå¿…é¡»æŠ›å‡ºå¼‚å¸¸
            logger.error(f"âœ— åˆ†é… Issue #{issue_num} ç»™ Copilot å¤±è´¥: {e}")
            logger.error("åˆ†é…å¤±è´¥æ„å‘³ç€ Copilot ä¸ä¼šè¢«è§¦å‘ï¼Œå°†æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘é‡è¯•")
            raise RuntimeError(f"æ— æ³•åˆ†é… Issue #{issue_num} ç»™ Copilotï¼Œè¯·æ£€æŸ¥æƒé™é…ç½®: {e}") from e

        return issue_num

    def _build_body(self, item: WorkItem) -> str:
        """æ„å»ºä»»åŠ¡è¯¦æƒ…éƒ¨åˆ†ï¼ˆTODO åˆ—è¡¨ï¼‰"""
        lines = [
            "### ğŸ“‹ ä»»åŠ¡è¯¦æƒ…",
            ""
        ]
        for todo in item.todos:
            lines.append(f"#### {todo.id_full} {todo.title}")
            lines.extend(todo.meta_lines)
            lines.append("")
        return "\n".join(lines)

    def _build_full_issue_body(self, item: WorkItem, task_details: str) -> str:
        """æ„å»ºå®Œæ•´çš„ Issue Bodyï¼ŒåŒ…å«æ‰§è¡ŒæŒ‡ä»¤å’Œä»»åŠ¡è¯¦æƒ…"""
        try:
            relative_path = item.file_path.relative_to(ROOT).as_posix()
        except ValueError:
            # å¦‚æœè·¯å¾„ä¸åœ¨ ROOT ä¸‹ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
            relative_path = item.file_path.as_posix()

        reference_files = f"""- `.github/copilot-instructions.md`
- `{relative_path}`
- `Stages/Stage-{item.stage_code}_*.md`
- `Project-Bible.md` (å¦‚ä¸å­˜åœ¨å°†è‡ªåŠ¨åˆ›å»º)
- `Risk-Ledger.md` (å¦‚ä¸å­˜åœ¨å°†è‡ªåŠ¨åˆ›å»º)"""

        instruction_body = ISSUE_BODY_TEMPLATE.format(
            task_overview=f"- **æ–‡ä»¶**: `{relative_path}`\n- **ä»»åŠ¡ID**: `{item.id_full}`\n- **TODOæ•°é‡**: {len(item.todos)}",
            stage_file=relative_path,
            stage_code=item.stage_code,
            plural="s" if item.is_batch else "",
            reference_files=reference_files
        )

        return f"{instruction_body}\n\n---\n\n{task_details}"

    def _wait_and_merge(self, item: WorkItem, issue_num: int) -> None:
        if self.args.dry_run:
            return

        github = self._require_github()
        reset_count = 0

        issue_start_time = time.time()
        wait_start_time = issue_start_time
        pr_create_time = None
        current_pr = None
        last_heartbeat = issue_start_time

        logger.info(f"å¼€å§‹ç›‘æ§ Issue #{issue_num}ï¼ŒPR è¶…æ—¶ {PR_TIMEOUT/3600:.1f}hï¼Œæœ€å¤§é‡ç½® {DEFAULT_MAX_PR_RESETS} æ¬¡")

        while True:
            elapsed_total = time.time() - issue_start_time

            # æ£€æŸ¥æ€»è¶…æ—¶ï¼šé˜²æ­¢ Issue å¡æ­»æ— é™ç­‰å¾…
            if elapsed_total >= self.args.issue_max_wait:
                raise RuntimeError(f"Issue #{issue_num} æ€»è¶…æ—¶ ({self.args.issue_max_wait/3600:.1f}h)")

            # æ£€æŸ¥ Issue æ˜¯å¦å·²å…³é—­
            try:
                issue_data = github.get_issue(issue_num)
                issue_state = issue_data.get("state")
                if issue_state == "closed":
                    # è­¦å‘Šï¼šIssue è¢«å…³é—­ä½†å¯èƒ½ PR æœªåˆå¹¶ï¼Œè®°å½•æ—¥å¿—
                    logger.info(f"âœ“ Issue #{issue_num} å·²å…³é—­")
                    if not current_pr:
                        logger.warning(f"è­¦å‘Šï¼šIssue #{issue_num} å·²å…³é—­ä½†æœªæ£€æµ‹åˆ°å…³è”çš„ PR")
                    return
            except Exception as e:
                logger.warning(f"è·å– Issue çŠ¶æ€å¤±è´¥ (å°†é‡è¯•): {e}")
                time.sleep(RETRY_SLEEP_SHORT)
                continue

            # è·å–æœ€æ–° PR
            try:
                pr_num = github.latest_pr_from_timeline(issue_num)
            except Exception as e:
                logger.warning(f"è·å– PR å¤±è´¥: {e}")
                time.sleep(RETRY_SLEEP_SHORT)
                continue

            # ä¿®å¤ï¼šå¦‚æœé•¿æ—¶é—´æ²¡æœ‰ PR åˆ›å»ºï¼Œè§¦å‘é‡ç½®
            if not pr_num:
                elapsed_since_start = time.time() - wait_start_time
                if elapsed_since_start > PR_WAIT_TIMEOUT:
                    if reset_count >= DEFAULT_MAX_PR_RESETS:
                        raise RuntimeError(
                            f"ç­‰å¾… PR åˆ›å»ºè¶…æ—¶ ({PR_WAIT_TIMEOUT/60:.1f}min)ï¼Œ"
                            f"ä¸”é‡ç½®æ¬¡æ•°å·²è¾¾ä¸Šé™ ({DEFAULT_MAX_PR_RESETS})ï¼ŒIssue #{issue_num} éœ€è¦äººå·¥ä»‹å…¥"
                        )

                    logger.warning(f"ç­‰å¾… PR åˆ›å»ºè¶…æ—¶ ({elapsed_since_start/60:.1f}min)ï¼Œè§¦å‘é‡ç½® (ç¬¬ {reset_count + 1}/{DEFAULT_MAX_PR_RESETS} æ¬¡)")
                    self._reset_issue(github, issue_num)
                    reset_count += 1
                    wait_start_time = time.time()  # ä»…é‡ç½® PR ç­‰å¾…è®¡æ—¶å™¨
                    time.sleep(RESET_WAIT_TIME)
                    continue

            if pr_num:
                # æ£€æµ‹åˆ°æ–° PR
                if current_pr != pr_num:
                    current_pr = pr_num
                    pr_create_time = time.time()
                    # æ³¨æ„ï¼šä¸é‡ç½® wait_start_timeï¼Œå®ƒä¸“é—¨ç”¨äºç­‰å¾… PR åˆ›å»ºè¶…æ—¶
                    logger.info(f"æ£€æµ‹åˆ° PR #{pr_num}")

                # æ£€æŸ¥ PR çŠ¶æ€
                try:
                    pr = github.get_pull(pr_num)
                except Exception as e:
                    logger.warning(f"è·å– PR çŠ¶æ€å¤±è´¥: {e}")
                    time.sleep(RETRY_SLEEP_SHORT)
                    continue

                # å¦‚æœå·²åˆå¹¶ï¼Œå®Œæˆ
                if pr.get("merged_at"):
                    logger.info(f"âœ“ PR #{pr_num} å·²åˆå¹¶")
                    return

                # å¦‚æœ PR è¢«å¤–éƒ¨å…³é—­ï¼ˆæœªåˆå¹¶ï¼‰ï¼Œé‡ç½®
                if pr.get("state") == "closed":
                    logger.warning(f"PR #{pr_num} å·²å…³é—­ä½†æœªåˆå¹¶")
                    if reset_count >= DEFAULT_MAX_PR_RESETS:
                        raise RuntimeError(f"PR é‡ç½®æ¬¡æ•°å·²è¾¾ä¸Šé™ ({DEFAULT_MAX_PR_RESETS})ï¼ŒIssue #{issue_num} éœ€è¦äººå·¥ä»‹å…¥")

                    # æ³¨æ„ï¼šreset_count ä» 0 å¼€å§‹ï¼Œæ‰€ä»¥è¿™æ˜¯ç¬¬ (reset_count + 1) æ¬¡é‡ç½®
                    logger.warning(f"é‡ç½®æµç¨‹ (ç¬¬ {reset_count + 1}/{DEFAULT_MAX_PR_RESETS} æ¬¡)")
                    self._reset_issue(github, issue_num)
                    reset_count += 1
                    current_pr = None
                    pr_create_time = None
                    wait_start_time = time.time()  # å…³é”®ä¿®å¤ï¼šé‡ç½®ç­‰å¾…è®¡æ—¶å™¨
                    time.sleep(RESET_WAIT_TIME)
                    continue

                # æ¡ä»¶1ï¼šæ£€æµ‹åˆ°å®Œæˆä¿¡å·ï¼Œç«‹å³æ ‡è®°ä¸º ready å¹¶åˆå¹¶ PR
                if check_copilot_signal(github, pr_num):
                    logger.info(f"âœ“ æ£€æµ‹åˆ° copilot_work_finished ä¿¡å·")

                    # å…³é”®ä¿®å¤ï¼šæ— è®ºå½“å‰çŠ¶æ€å¦‚ä½•ï¼Œéƒ½å°è¯•æ ‡è®°ä¸º ready
                    # å› ä¸º Copilot å®Œæˆå PR å¯èƒ½å¤„äº "ready for review" çŠ¶æ€
                    # éœ€è¦æ˜¾å¼è°ƒç”¨ gh pr ready æ‰èƒ½åˆå¹¶
                    logger.info(f"å°è¯•å°† PR #{pr_num} æ ‡è®°ä¸º Ready çŠ¶æ€...")
                    try:
                        github.mark_pr_ready(pr_num)
                        logger.info(f"âœ“ å·²å°† PR #{pr_num} æ ‡è®°ä¸º Ready")
                        time.sleep(PR_READY_WAIT)
                    except Exception as e:
                        # å¦‚æœ PR å·²ç»æ˜¯ ready çŠ¶æ€ï¼Œå‘½ä»¤å¯èƒ½ä¼šå¤±è´¥ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                        logger.debug(f"æ ‡è®° Ready æ—¶å‡ºç°å¼‚å¸¸ï¼ˆå¯èƒ½ PR å·²æ˜¯ Ready çŠ¶æ€ï¼‰: {e}")

                    try:
                        github.merge_pull(pr_num)
                        logger.info(f"âœ“ PR #{pr_num} åˆå¹¶æˆåŠŸ")
                        return
                    except Exception as e:
                        # å†æ¬¡ç¡®è®¤æ˜¯å¦å·²åˆå¹¶
                        try:
                            pr_status = github.get_pull(pr_num)
                            if pr_status.get("merged_at"):
                                logger.info(f"âœ“ PR #{pr_num} å·²åˆå¹¶")
                                return
                        except Exception:
                            pass

                        # åˆå¹¶å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é‡ç½®
                        logger.error(f"åˆå¹¶ PR #{pr_num} å¤±è´¥: {e}")
                        if reset_count >= DEFAULT_MAX_PR_RESETS:
                            raise RuntimeError(f"åˆå¹¶å¤±è´¥ä¸”é‡ç½®æ¬¡æ•°å·²è¾¾ä¸Šé™ ({DEFAULT_MAX_PR_RESETS})ï¼ŒIssue #{issue_num} éœ€è¦äººå·¥ä»‹å…¥: {e}") from e

                        # è§¦å‘é‡ç½®ï¼Œè®© Copilot é‡æ–°å¤„ç†
                        logger.warning(f"å°†é‡ç½®æµç¨‹å¹¶è®© Copilot é‡è¯• (ç¬¬ {reset_count + 1}/{DEFAULT_MAX_PR_RESETS} æ¬¡)")
                        try:
                            merge_fail_comment = f"""âŒ **PR åˆå¹¶å¤±è´¥**

é”™è¯¯ä¿¡æ¯ï¼š{str(e)[:200]}

å·²å…³é—­æ­¤ PR å¹¶è§¦å‘ Issue #{issue_num} çš„é‡ç½®æµç¨‹ã€‚
é‡ç½®æ¬¡æ•°ï¼š{reset_count + 1}/{DEFAULT_MAX_PR_RESETS}
"""
                            github.comment_issue(pr_num, merge_fail_comment)
                            github.close_pr(pr_num, delete_branch=True)
                        except Exception:
                            pass  # å…³é—­å¤±è´¥ä¹Ÿç»§ç»­
                        self._reset_issue(github, issue_num)
                        reset_count += 1
                        current_pr = None
                        pr_create_time = None
                        wait_start_time = time.time()
                        time.sleep(RESET_WAIT_TIME)
                        continue

                # æ¡ä»¶2ï¼šPR è¶…æ—¶ï¼Œé‡ç½®æµç¨‹
                if pr_create_time:
                    elapsed = time.time() - pr_create_time
                    if elapsed > PR_TIMEOUT:
                        if reset_count >= DEFAULT_MAX_PR_RESETS:
                            logger.error(f"PR #{pr_num} è¶…æ—¶ ({elapsed/3600:.1f}h)ï¼Œå·²è¾¾æœ€å¤§é‡ç½®æ¬¡æ•°")
                            raise RuntimeError(f"PR è¶…æ—¶ä¸”é‡ç½®æ¬¡æ•°å·²è¾¾ä¸Šé™ ({DEFAULT_MAX_PR_RESETS})ï¼ŒIssue #{issue_num} éœ€è¦äººå·¥ä»‹å…¥")

                        logger.warning(f"PR #{pr_num} è¶…æ—¶ ({elapsed/3600:.1f}h / {PR_TIMEOUT/3600:.1f}h)ï¼Œå‡†å¤‡é‡ç½®æµç¨‹ (ç¬¬ {reset_count + 1}/{DEFAULT_MAX_PR_RESETS} æ¬¡)")

                        # å…³é—­è¶…æ—¶ PRï¼ˆæ³¨æ„ï¼šå…³é—­ PR ä¸ä¼šå…³é—­ Issueï¼ŒIssue ä»ç„¶ä¿æŒ openï¼‰
                        try:
                            # å…ˆæ·»åŠ è¯„è®ºè¯´æ˜è¶…æ—¶åŸå› ï¼Œæ–¹ä¾¿åæœŸå®¡è®¡
                            timeout_comment = f"""ğŸ•’ **PR è¶…æ—¶è‡ªåŠ¨å…³é—­**

Copilot å¤„ç†æ—¶é—´è¶…è¿‡ {PR_TIMEOUT/3600:.1f} å°æ—¶ï¼Œè‡ªåŠ¨å…³é—­æ­¤ PRã€‚
å·²è§¦å‘ Issue #{issue_num} çš„é‡ç½®æµç¨‹ï¼ŒCopilot å°†é‡æ–°å¤„ç†ä»»åŠ¡ã€‚

é‡ç½®æ¬¡æ•°ï¼š{reset_count + 1}/{DEFAULT_MAX_PR_RESETS}
"""
                            github.comment_issue(pr_num, timeout_comment)  # PR ä¹Ÿæ˜¯ä¸€ç§ Issue
                            github.close_pr(pr_num, delete_branch=True)
                            logger.info(f"âœ“ å·²å…³é—­è¶…æ—¶ PR #{pr_num} å¹¶æ·»åŠ è¯´æ˜è¯„è®º")
                        except Exception as e:
                            logger.warning(f"å…³é—­ PR å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œé‡ç½®ï¼‰: {e}")

                        self._reset_issue(github, issue_num)
                        reset_count += 1
                        current_pr = None
                        pr_create_time = None
                        wait_start_time = time.time()
                        logger.info(f"å·²è§¦å‘é‡ç½®ï¼Œç­‰å¾… {RESET_WAIT_TIME} ç§’åç»§ç»­ç›‘æ§")
                        time.sleep(RESET_WAIT_TIME)
                        continue

            # å¿ƒè·³æ—¥å¿—
            current_time = time.time()
            if current_time - last_heartbeat >= HEARTBEAT_INTERVAL:
                elapsed_mins = (current_time - issue_start_time) / 60
                reset_suffix = f" [é‡ç½®:{reset_count}/{DEFAULT_MAX_PR_RESETS}]" if reset_count > 0 else ""

                if pr_num and pr_create_time:
                    pr_elapsed = (current_time - pr_create_time) / 60
                    pr_remaining = (PR_TIMEOUT - (current_time - pr_create_time)) / 60
                    indicator = "â°" if pr_remaining < 30 else "â³"
                    status = f"ç­‰å¾…ä¿¡å· (PR #{pr_num}, {pr_elapsed:.0f}/{PR_TIMEOUT/60:.0f}min, å‰©ä½™{pr_remaining:.0f}min){reset_suffix} {indicator}"
                else:
                    wait_elapsed = (current_time - wait_start_time) / 60
                    wait_remaining = (PR_WAIT_TIMEOUT - (current_time - wait_start_time)) / 60
                    status = f"ç­‰å¾… PR ({wait_elapsed:.0f}/{PR_WAIT_TIMEOUT/60:.0f}min, å‰©ä½™{wait_remaining:.0f}min){reset_suffix}"

                logger.info(f"ğŸ’“ [{elapsed_mins:.0f}min] {status}")
                last_heartbeat = current_time

            time.sleep(self.args.poll_interval)

    def _reset_issue(self, github: GitHubClient, issue_num: int) -> None:
        """é‡ç½® Issueï¼šé€šè¿‡ unassign + assign è§¦å‘ Copilot é‡æ–°å¤„ç†"""
        try:
            # æ£€æŸ¥ Issue çŠ¶æ€ï¼Œå¦‚æœå·²å…³é—­åˆ™ä¸é‡ç½®
            issue_data = github.get_issue(issue_num)
            if issue_data.get("state") == "closed":
                logger.warning(f"Issue #{issue_num} å·²å…³é—­ï¼Œè·³è¿‡é‡ç½®")
                return

            # è·å–å½“å‰åˆ†é…çš„ç”¨æˆ·åˆ—è¡¨
            assignees = {
                (assignee.get("login") or "").lower()
                for assignee in issue_data.get("assignees", []) if assignee
            }

            # å…³é”®ä¿®å¤ï¼šå…ˆ unassign Copilotï¼ˆå¦‚æœå·²åˆ†é…ï¼‰ï¼Œå†é‡æ–° assign
            # è¿™æ˜¯è§¦å‘ Copilot é‡æ–°å¤„ç†çš„æ­£ç¡®æ–¹å¼
            if COPILOT_USERNAME in assignees:
                logger.info(f"æ£€æµ‹åˆ° Copilot å·²åˆ†é…ï¼Œå…ˆå–æ¶ˆåˆ†é…ä»¥è§¦å‘é‡æ–°å¤„ç†")
                github.remove_assignees(issue_num, COPILOT_ASSIGNEES)
                time.sleep(2)  # ç»™ GitHub ä¸€ç‚¹æ—¶é—´å¤„ç† unassign

            # é‡æ–°åˆ†é… Copilot
            logger.info(f"é‡æ–°åˆ†é… Issue #{issue_num} ç»™ Copilot")
            github.add_assignees(issue_num, COPILOT_ASSIGNEES)
            logger.info(f"âœ“ å·²è§¦å‘ Copilot é‡æ–°å¤„ç† Issue #{issue_num}")

        except Exception as e:
            logger.error(f"é‡ç½® Issue å¤±è´¥: {e}")
            raise

    # ==================== å…¥å£ ====================

def main() -> int:
    parser = argparse.ArgumentParser(description="Auto Copilot Pipeline - è‡ªåŠ¨ç»­ä¼ ç‰ˆæœ¬")
    parser.add_argument("--poll-interval", type=int, default=DEFAULT_POLL_INTERVAL,
                        help="è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--issue-max-wait", type=int, default=DEFAULT_MAX_WAIT,
                        help="å•ä¸ª Issue æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--issue-batch-size", type=int, default=DEFAULT_BATCH_SIZE,
                        help="æ¯ä¸ª Issue åŒ…å«çš„ TODO æ•°é‡")
    parser.add_argument("--task-max-retries", type=int, default=DEFAULT_TASK_MAX_RETRIES,
                        help="å•ä¸ªå·¥ä½œé¡¹å¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument("--task-retry-wait", type=int, default=DEFAULT_TASK_RETRY_WAIT,
                        help="é¦–æ¬¡é‡è¯•å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œä¹‹åæŒ‰é‡è¯•åºå·é€’å¢")
    parser.add_argument("--dry-run", action="store_true",
                        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸åˆ›å»ºå®é™… Issue")
    parser.add_argument("--from-beginning", action="store_true",
                        help="å¼ºåˆ¶ä»å¤´å¼€å§‹ï¼Œå¿½ç•¥ GitHub Issues ä¸­çš„è¿›åº¦")
    parser.add_argument("--repo", type=str,
                        help="æ‰‹åŠ¨æŒ‡å®šä»“åº“ (æ ¼å¼: owner/repo)ï¼Œè¦†ç›–è‡ªåŠ¨æ£€æµ‹")
    args = parser.parse_args()

    # éªŒè¯å‚æ•°åˆç†æ€§
    if args.poll_interval < 1:
        logger.error("è½®è¯¢é—´éš”å¿…é¡»è‡³å°‘ä¸º 1 ç§’")
        return 1
    if args.issue_max_wait < 60:
        logger.error("Issue è¶…æ—¶å¿…é¡»è‡³å°‘ä¸º 60 ç§’")
        return 1
    if args.issue_batch_size < 1:
        logger.error("æ‰¹æ¬¡å¤§å°å¿…é¡»è‡³å°‘ä¸º 1")
        return 1
    if args.task_max_retries < 1:
        logger.error("ä»»åŠ¡æœ€å¤§é‡è¯•æ¬¡æ•°å¿…é¡»è‡³å°‘ä¸º 1")
        return 1

    if args.repo:
        if "/" not in args.repo:
            logger.error("ä»“åº“æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º owner/repo")
            return 1
        owner, repo = args.repo.split("/", 1)
    else:
        try:
            owner, repo = resolve_repo()
        except RuntimeError as e:
            if args.dry_run:
                logger.warning(f"DRY RUN æ¨¡å¼ä¸”æ— æ³•æ£€æµ‹ä»“åº“: {e}")
                logger.warning("å°†ä½¿ç”¨æ¨¡æ‹Ÿä»“åº“ dummy/repo ç»§ç»­è¿è¡Œ")
                owner, repo = "dummy", "repo"
            else:
                raise

    logger.info("="*80)
    logger.info("Auto Copilot Pipeline - é…ç½®")
    logger.info("="*80)
    logger.info(f"ä»“åº“: {owner}/{repo}")
    logger.info(f"è½®è¯¢é—´éš”: {args.poll_interval}ç§’")
    logger.info(f"Issue è¶…æ—¶: {args.issue_max_wait}ç§’ ({args.issue_max_wait/3600:.1f}å°æ—¶)")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.issue_batch_size}")
    logger.info(f"ä»»åŠ¡æœ€å¤§é‡è¯•: {args.task_max_retries} æ¬¡ (åˆå§‹ç­‰å¾… {args.task_retry_wait}ç§’)")
    if args.dry_run:
        logger.info("æ¨¡å¼: DRY RUN (é¢„è§ˆ)")
    logger.info("="*80)

    try:
        github: Optional[GitHubClient] = None
        if args.dry_run:
            logger.info("Dry-run æ¨¡å¼ï¼šè·³è¿‡ GitHub å®¢æˆ·ç«¯åˆå§‹åŒ–")
        else:
            github = GitHubClient(owner, repo)
        ensure_core_documents()
        pipeline = Pipeline(github, args)

        iteration = 0
        while True:
            try:
                iteration += 1

                completed_ids: set[str] = set()
                if not args.from_beginning:
                    completed_ids = pipeline.get_recent_completed_todos()

                work_items = iter_work_items(TODO_ROOT, args.issue_batch_size, completed_ids)

                if not work_items:
                    if iteration == 1:
                        logger.info("âœ“ æ‰€æœ‰ TODO å·²å®Œæˆï¼Œæ— éœ€è¿›ä¸€æ­¥æ“ä½œã€‚")
                    # å¦‚æœæ˜¯æŒç»­è¿è¡Œæ¨¡å¼ï¼Œä¸”æ²¡æœ‰æ–°ä»»åŠ¡ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´å†æ‰«æ
                    if not args.dry_run:
                        logger.info(f"æš‚æ— å¾…åŠä»»åŠ¡ï¼Œ{args.poll_interval} ç§’åé‡æ–°æ‰«æ...")
                        time.sleep(args.poll_interval)
                        continue
                    break

                if iteration > 1:
                    logger.info("\n" + "="*80)
                    logger.info(f"è‡ªåŠ¨ç»­ä¼ ï¼šæ£€æµ‹åˆ°æ–°çš„ä»»åŠ¡æ‰¹æ¬¡ (ç¬¬ {iteration} è½®)")
                    logger.info("="*80)

                pipeline.run(work_items)

                if args.dry_run:
                    logger.info("Dry-run æ¨¡å¼ï¼šé¦–è½®ä»»åŠ¡é¢„è§ˆå®Œæˆï¼Œè‡ªåŠ¨é€€å‡ºã€‚")
                    break

            except Exception as e:
                logger.error(f"\nâœ— æµæ°´çº¿æ‰§è¡Œå‡ºé”™ (ç¬¬ {iteration} è½®): {e}", exc_info=True)
                if args.dry_run:
                    raise  # Dry run æ¨¡å¼ä¸‹ç›´æ¥æŠ¥é”™é€€å‡º

                # æ— äººå€¼å®ˆæ¨¡å¼ï¼šç­‰å¾…åé‡è¯•
                wait_time = MAIN_ERROR_WAIT
                logger.info(f"å°†åœ¨ {wait_time} ç§’åè‡ªåŠ¨é‡è¯•...")
                time.sleep(wait_time)
                continue

        logger.info("\n" + "="*80)
        logger.info("âœ“ æµæ°´çº¿æ‰§è¡ŒæˆåŠŸå®Œæˆ")
        logger.info("="*80)
        return 0
    except KeyboardInterrupt:
        logger.warning("\nâš  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 130
    except Exception as e:
        logger.error(f"\nâœ— è‡´å‘½é”™è¯¯: {e}", exc_info=True)
        return 1

def signal_handler(signum: int, frame: Any) -> None:
    """ä¼˜é›…é€€å‡ºä¿¡å·å¤„ç†å™¨"""
    logger.warning(f"\nâš  æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    logger.info("æç¤ºï¼šå¯ä»¥ä½¿ç”¨ --from-beginning é‡æ–°å¼€å§‹ï¼Œæˆ–ç›´æ¥è¿è¡Œä»¥ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­")
    sys.exit(130)

if __name__ == "__main__":
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆCtrl+C ç­‰ï¼‰
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)

    sys.exit(main())
